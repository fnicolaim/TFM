{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM creation notebook\n",
    "\n",
    "Note: This notebook was executed in google colab since it provided easier instalation of ml packages plus the odmatrices where already grouped at a municipality level thus no privacy data was compromised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "\n",
    "def mean_relative_error(inv_y, inv_yhat, epsilon=1e-10):\n",
    "    if len(inv_y) != len(inv_yhat):\n",
    "        raise ValueError(\"Lists must have the same length\")\n",
    "\n",
    "    total_error = 0\n",
    "    count = 0\n",
    "    for y, yhat in zip(inv_y, inv_yhat):\n",
    "        if y != 0:  # Only compute the relative error if the actual value is not zero\n",
    "            total_error += abs(y - yhat) / (abs(y) + epsilon)\n",
    "            count += 1\n",
    "    mre = total_error / count if count != 0 else 0\n",
    "    return mre\n",
    "\n",
    "\n",
    "def plt_train_plot(history, odp):\n",
    "  # plot history\n",
    "  pyplot.plot(history.history['loss'], label='train')\n",
    "  pyplot.plot(history.history['val_loss'], label='test')\n",
    "  pyplot.legend()\n",
    "\n",
    "  # Adding y-axis grid\n",
    "  pyplot.grid(axis='y')\n",
    "  # Adding titles and labels\n",
    "  pyplot.title(f'Learning curves {odp} model')\n",
    "  pyplot.xlabel('Epochs')\n",
    "  pyplot.ylabel('Loss')\n",
    "  # Saving the plot as a PNG file\n",
    "  pyplot.savefig(rf\"/content/drive/MyDrive/TFM/TPP/entrenamiento_{odp}.png\", dpi=300, bbox_inches='tight')\n",
    "  pyplot.show()\n",
    "\n",
    "\n",
    "def plotly_train_plot(history, odp):\n",
    "  trace_train = go.Scatter(\n",
    "      x=list(range(len(history.history['loss']))),\n",
    "      y=history.history['loss'],\n",
    "      mode='lines',\n",
    "      name='train'\n",
    "  )\n",
    "  trace_test = go.Scatter(\n",
    "      x=list(range(len(history.history['val_loss']))),\n",
    "      y=history.history['val_loss'],\n",
    "      mode='lines',\n",
    "      name='test'\n",
    "  )\n",
    "  layout = go.Layout(\n",
    "      title=f'Learning curves {odp} model',\n",
    "      xaxis=dict(title='Epochs'),\n",
    "      yaxis=dict(title='Loss', gridcolor='lightgray'),\n",
    "      plot_bgcolor='rgba(0,0,0,0)'\n",
    "  )\n",
    "  # Create a figure from data and layout, and plot the figure\n",
    "  fig = go.Figure(data=[trace_train, trace_test], layout=layout)\n",
    "  fig.update_layout(legend=dict(yanchor=\"top\", y=0.99,xanchor=\"left\",x=0.99),\n",
    "                    title_x=0.5, title_y=0.9,)\n",
    "  fig.write_image(rf\"/content/drive/MyDrive/TFM/TPP/entrenamiento_{odp}.png\")\n",
    "  \n",
    "\n",
    "\n",
    "def train_model(train_X, train_y, test_X, test_y, hidden_size):\n",
    "    model = Sequential()\n",
    "    # early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.add(LSTM(hidden_size, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1, activation=\"swish\"))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    history = model.fit(train_X, train_y, epochs=10, batch_size=32, validation_data=(test_X, test_y),\n",
    "                        verbose=0, shuffle=False,)\n",
    "                        # callbacks=[early_stop])\n",
    "    return history.history['loss'][-1], history.history['val_loss'][-1]  # Return validation loss of the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch for hidden state over madrid municipality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and select just one od pair\n",
    "dataset = pd.read_csv(r\"/content/drive/MyDrive/TFM/TPP/final_odm_with_weather.csv\", dtype=\"str\")\n",
    "dataset = dataset.astype({'trips': float, 'temperature_2m (Â°C)': float, 'rain (mm)': float, 'snowfall (cm)': float})\n",
    "dataset[\"od\"] = dataset[\"origen\"]+ \"|\" + dataset[\"destino\"]\n",
    "\n",
    "# Format\n",
    "dataset[\"fecha\"] = pd.to_datetime(dataset[\"fecha\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "dataset = dataset.drop(columns=[\"origen\",\"destino\"])\n",
    "# Remove first three hours and 31/10/2021\n",
    "dataset = dataset[(dataset[\"fecha\"]>datetime(2021,1,1,3)) &\n",
    "                  (dataset[\"fecha\"]<datetime(2021,10,31,4))]\n",
    "# dataset = dataset.set_index(\"fecha\")\n",
    "\n",
    "# Make a copy to further merge\n",
    "weather = dataset.drop(columns=[\"trips\",\"od\"]).drop_duplicates()\n",
    "\n",
    "df = dataset[dataset.od==\"28079|28079\"].copy()#.drop(columns=[\"od\"])\n",
    "\n",
    "# Merge with weather to fill 0 trips rows\n",
    "df = df[[\"fecha\",\"trips\"]].merge(weather, on=\"fecha\", how=\"right\")\n",
    "df[\"trips\"] = df[\"trips\"].fillna(0)\n",
    "\n",
    "# Fechas\n",
    "fechas = df[\"fecha\"].tolist()\n",
    "# Sort columns\n",
    "df = df[[\"trips\", \"temperature_2m (Â°C)\", \"rain (mm)\",\"snowfall (cm)\"]]\n",
    "# Select values\n",
    "values = df.values\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# scaled = values\n",
    "# specify the number of lag hours\n",
    "n_hours = 3\n",
    "n_features = 4 # ALL COLUMNS IN ORIGINAL DF\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_hours, 1)\n",
    "print(reframed.shape)\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 180 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "fechas_train = fechas[:n_train_hours]\n",
    "fechas_test = fechas[n_train_hours:]\n",
    "# # split into input and outputs\n",
    "n_obs = n_hours * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features] # Todas las filas y columnas (t-x), col var1(t)\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "# Train_X looks like (withouth column names)\n",
    "# var1(t-3)\tvar2(t-3)\tvar3(t-3)\tvar4(t-3)\tvar1(t-2)\tvar2(t-2)\tvar3(t-2) var4(t-2)\tvar1(t-1)\tvar2(t-1)\tvar3(t-1)\tvar4(t-1)\n",
    "# 3\t1.0\t1.5\t0.1\t0.0\t7.0\t0.600000\t0.1\t0.0\t26.0\t0.600000\t0.2\t0.0\n",
    "#\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "display(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "file_path=r\"/content/drive/MyDrive/TFM/TPP/gridsearch_28079.csv\"\n",
    "with open(file_path,'w') as f:\n",
    "  f.write(\"size,loss,val_loss\\n\")\n",
    "\n",
    "# Optimize hidden state value\n",
    "for size in tqdm([v for v in range(1,50)]):\n",
    "    # Train\n",
    "    train_loss, val_loss = train_model(train_X, train_y, test_X, test_y, size)\n",
    "    # Save (in append so no content is loss by interruption)\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(f\"{size},{train_loss},{val_loss}\\n\")\n",
    "\n",
    "grid_search = pd.read_csv('/content/drive/MyDrive/TFM/TPP/gridsearch_28079.csv')\n",
    "\n",
    "# Load and explore gridsearch results\n",
    "grid_search = pd.read_csv('/content/drive/MyDrive/TFM/TPP/gridsearch_28079.csv')\n",
    "display(grid_search.loc[grid_search.loss==grid_search.loss.min()])\n",
    "display(grid_search.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs hidden state size\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=grid_search[\"size\"], y=grid_search[\"loss\"],\n",
    "                    mode='lines',\n",
    "                    name='loss'))\n",
    "fig.add_trace(go.Scatter(x=grid_search[\"size\"], y=grid_search[\"val_loss\"],\n",
    "                    mode='lines',\n",
    "                    name='val_loss'))\n",
    "\n",
    "fig.update_layout(#title_text='Loss and validation loss versus hidden state size', title_x=0.5, title_y=0.9,\n",
    "                  xaxis_title='Hidden State Size',\n",
    "                  yaxis_title='Loss Value',\n",
    "                  yaxis=dict(title='Loss', gridcolor='lightgray'),\n",
    "                  plot_bgcolor='rgba(0,0,0,0)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, train and save each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and select just one od pair\n",
    "dataset = pd.read_csv(r\"/content/drive/MyDrive/TFM/TPP/final_odm_with_weather.csv\", dtype=\"str\")\n",
    "dataset = dataset.astype({'trips': float, 'temperature_2m (Â°C)': float, 'rain (mm)': float, 'snowfall (cm)': float})\n",
    "dataset[\"od\"] = dataset[\"origen\"]+ \"|\" + dataset[\"destino\"]\n",
    "\n",
    "# Format\n",
    "dataset[\"fecha\"] = pd.to_datetime(dataset[\"fecha\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "dataset = dataset.drop(columns=[\"origen\",\"destino\"])\n",
    "# Remove first three hours and 31/10/2021\n",
    "dataset = dataset[(dataset[\"fecha\"]>datetime(2021,1,1,3)) &\n",
    "                  (dataset[\"fecha\"]<datetime(2021,10,31,4))]\n",
    "# dataset = dataset.set_index(\"fecha\")\n",
    "\n",
    "# Make a copy to further merge\n",
    "weather = dataset.drop(columns=[\"trips\",\"od\"]).drop_duplicates()\n",
    "\n",
    "# OD PAIRS TO STUDY\n",
    "odps = [\n",
    "    \"28079|28079\",\n",
    "    \"28074|28074\",\n",
    "    \"28005|28005\",\n",
    "    \"28092|28092\",\n",
    "    \"28065|28065\",\n",
    "    \"28058|28058\",\n",
    "    \"28007|28007\",\n",
    "    \"28148|28148\",\n",
    "    \"28079|28007\",\n",
    "    \"28079|28092\",\n",
    "        ]\n",
    "\n",
    "models_dict = {}\n",
    "for odp in odps:\n",
    "  df = dataset[dataset.od==odp].copy()#.drop(columns=[\"od\"])\n",
    "\n",
    "  # Merge with weather to fill 0 trips rows\n",
    "  df = df[[\"fecha\",\"trips\"]].merge(weather, on=\"fecha\", how=\"right\")\n",
    "  df[\"trips\"] = df[\"trips\"].fillna(0)\n",
    "\n",
    "  # Fechas\n",
    "  fechas = df[\"fecha\"].tolist()\n",
    "  # Sort columns\n",
    "  df = df[[\"trips\", \"temperature_2m (Â°C)\", \"rain (mm)\",\"snowfall (cm)\"]]\n",
    "  # Select values\n",
    "  values = df.values\n",
    "  # ensure all data is float\n",
    "  values = values.astype('float32')\n",
    "\n",
    "  # ==== Train model pipeline =====\n",
    "  # normalize features\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  scaled = scaler.fit_transform(values)\n",
    "  # specify the number of lag hours\n",
    "  n_hours = 3\n",
    "  n_features = 4 # Number of cols in original df\n",
    "  # frame as supervised learning\n",
    "  reframed = series_to_supervised(scaled, n_hours, 1)\n",
    "  # split into train and test sets\n",
    "  values = reframed.values\n",
    "  n_train_hours = 180 * 24\n",
    "  train = values[:n_train_hours, :]\n",
    "  test = values[n_train_hours:, :]\n",
    "  fechas_train = fechas[:n_train_hours]\n",
    "  fechas_test = fechas[n_train_hours:]\n",
    "  # # split into input and outputs\n",
    "  n_obs = n_hours * n_features\n",
    "  train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "  test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "  # reshape input to be 3D [samples, timesteps, features]\n",
    "  train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "  test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "\n",
    "  # design network\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(45, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "  model.add(Dense(1, activation=\"swish\"))\n",
    "  # model.add(Dense(1))\n",
    "  model.compile(loss='mae', optimizer='adam')\n",
    "  # fit network\n",
    "  early_stop = EarlyStopping(monitor='val_loss', patience=10,\n",
    "                             restore_best_weights=True)\n",
    "  history = model.fit(train_X, train_y, epochs=300, batch_size=32,\n",
    "                      validation_data=(test_X, test_y), verbose=2,\n",
    "                      shuffle=False, callbacks=[early_stop])\n",
    "###########\n",
    "  # Plot learning curves\n",
    "  plotly_train_plot(history, odp)\n",
    "  # make a prediction\n",
    "  yhat = model.predict(test_X)\n",
    "  test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "  # invert scaling for forecast\n",
    "  inv_yhat = concatenate((yhat, test_X[:, -n_features+1:]), axis=1)\n",
    "  inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "  inv_yhat = inv_yhat[:,0]\n",
    "  # invert scaling for actual\n",
    "  test_y = test_y.reshape((len(test_y), 1))\n",
    "  inv_y = concatenate((test_y, test_X[:, -n_features+1:]), axis=1)\n",
    "  inv_y = scaler.inverse_transform(inv_y)\n",
    "  inv_y = inv_y[:,0]\n",
    "  # calculate RMSE and mre (mean relative error)\n",
    "  rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "  mre = mean_relative_error(inv_y, inv_yhat)\n",
    "\n",
    "  print(f'Test RMSE {odp}: {round(rmse,3)}')\n",
    "  print(f'Test MRE {odp}: {round(mre,2)}')\n",
    "  models_dict[odp] = {\"model\": history, \"rmse_test\": rmse, \"mre_test\": mre,\n",
    "                      \"#_epochs\": early_stop.stopped_epoch}\n",
    "\n",
    "  # calculate RMSE by month\n",
    "  errors = {}\n",
    "  for i in range(4):\n",
    "    months = {0:\"july\",1:\"august\",2:\"september\",3:\"october\"}\n",
    "    rmse = sqrt(mean_squared_error(inv_y[i*30*24:(i+1)*30*24],\n",
    "                                   inv_yhat[i*30*24:(i+1)*30*24]))\n",
    "    mre = mean_relative_error(inv_y[i*30*24:(i+1)*30*24],\n",
    "                                   inv_yhat[i*30*24:(i+1)*30*24])\n",
    "    errors[\"rmse_\" + months[i]] = rmse\n",
    "    errors[\"mre_\" + months[i]] = mre\n",
    "  models_dict[odp].update(errors)\n",
    "\n",
    "  # Plots (we skipp first N-hours since they dont have predicted val)\n",
    "  _d = pd.DataFrame()\n",
    "  _d[\"Date\"] = fechas_test[n_hours:]\n",
    "  _d[\"Trips\"] = inv_y\n",
    "  _d[\"Legend\"] = \"Expected\"\n",
    "  _d2 = pd.DataFrame()\n",
    "  _d2[\"Date\"] = fechas_test[n_hours:]\n",
    "  _d2[\"Trips\"] = inv_yhat\n",
    "  _d2[\"Legend\"] = \"Prediction\"\n",
    "  _d = pd.concat([_d, _d2], ignore_index=True)\n",
    "  fig = px.line(_d, x=\"Date\", y=\"Trips\", color=\"Legend\")\n",
    "  fig.update_layout(height=600, width=1200, plot_bgcolor='rgba(0,0,0,0)',\n",
    "                    yaxis=dict(gridcolor='lightgray'))\n",
    "  fig.write_html(rf\"/content/drive/MyDrive/TFM/TPP/grafico_{odp}.html\")\n",
    "  fig.write_image(rf\"/content/drive/MyDrive/TFM/TPP/grafico_{odp}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results exploration\n",
    "\n",
    "1) RMSE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the data\n",
    "models_dict_copy = copy.deepcopy(models_dict)\n",
    "\n",
    "# Remove the 'model' key from each inner dictionary in the copied data\n",
    "for key in models_dict_copy:\n",
    "    if 'model' in models_dict_copy[key]:\n",
    "        del models_dict_copy[key]['model']\n",
    "\n",
    "# Convert data_copy to a pandas DataFrame\n",
    "models_dict_df = pd.DataFrame.from_dict(models_dict_copy, orient='index')\n",
    "models_dict_df = models_dict_df.reset_index().rename(columns={\"rmse_test\":\"rmse\", \"index\":\"OD pair\"})\n",
    "display(models_dict_df[[\"OD pair\"]+[c for c in models_dict_df.columns if \"rmse\" in c]].round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Training epochs per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_df.rename(columns={r\"#_epochs\":\"total epochs\"})[[\"OD pair\", \"total epochs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditional metadata\n",
    "\n",
    "weather data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.rename(columns={\"fecha\":\"Date\"})\n",
    "fig = make_subplots(rows=3, cols=1,)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weather[\"Date\"], y=weather[\"temperature_2m (Â°C)\"],line=dict(color='rgb(99,110,250)')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weather[\"Date\"], y=weather[\"snowfall (cm)\"],line=dict(color='rgb(99,110,250)')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=weather[\"Date\"], y=weather[\"rain (mm)\"],line=dict(color='rgb(99,110,250)')),\n",
    "    row=3, col=1\n",
    ")\n",
    "fig['layout']['yaxis']['title']='temperature (CÂ°)'\n",
    "fig['layout']['yaxis2']['title']='snowfall (cm)'\n",
    "fig['layout']['yaxis3']['title']='rain (mm)'\n",
    "\n",
    "fig.update_layout(height=600, width=1200, showlegend=False, plot_bgcolor='rgba(0, 0, 0, 0)')\n",
    "fig.update_yaxes(\n",
    "    mirror=True,\n",
    "    ticks='outside',\n",
    "    showline=True,\n",
    "    # linecolor='black',\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
